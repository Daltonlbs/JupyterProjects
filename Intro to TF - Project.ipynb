{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cbd3866",
   "metadata": {},
   "source": [
    "#Não tem o arquivo de referência <br><br>\n",
    "\n",
    "https://github.com/ivanovitchm/deeplearning/blob/main/weeks_02_03/Notebooks/Week%2002%20Task%2002%20-%20Introduction%20to%20TF.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6282cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce7798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # load the train data\n",
    "    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "\n",
    "    # your train set features\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) \n",
    "\n",
    "    # your train set labels\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) \n",
    "\n",
    "    \n",
    "    # load the test data\n",
    "    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
    "\n",
    "    # your test set features\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) \n",
    "\n",
    "    # your test set labels  \n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) \n",
    "\n",
    "    # the list of classes\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) \n",
    "\n",
    "    # reshape the test data\n",
    "    train_set_y_orig = train_set_y_orig.reshape((train_set_y_orig.shape[0],1))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((test_set_y_orig.shape[0],1))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ba763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a972e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1)\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1)\n",
    "\n",
    "# Standardize the dataset\n",
    "train_set_x = train_set_x_flatten/255\n",
    "test_set_x = test_set_x_flatten/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x  shape: \" + str(test_set_x.shape))\n",
    "print (\"test_set_y  shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0469d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a sample data\n",
    "index = 13\n",
    "plt.imshow(train_set_x_orig[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a source dataset from your training data\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_set_x,train_set_y))\n",
    "dataset = dataset.shuffle(buffer_size=64).batch(32)\n",
    "\n",
    "# Instantiate a simple classification model\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(8, activation=tf.nn.relu, dtype='float64'),\n",
    "  tf.keras.layers.Dense(8, activation=tf.nn.relu, dtype='float64'),\n",
    "  tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, dtype='float64')\n",
    "])\n",
    "\n",
    "# Instantiate a logistic loss function that expects integer targets.\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Instantiate an accuracy metric.\n",
    "accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb696af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):    \n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x, y) in enumerate(dataset):\n",
    "      # Open a GradientTape.\n",
    "      with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass.\n",
    "        logits = model(x)\n",
    "\n",
    "        # Loss value for this batch.\n",
    "        loss_value = loss(y, logits)\n",
    "          \n",
    "      # Get gradients of loss wrt the weights.\n",
    "      gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "      # Update the weights of our linear layer.\n",
    "      optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "      # Update the running accuracy.\n",
    "      accuracy.update_state(y, logits)\n",
    "\n",
    "    # Logging.\n",
    "    if i % 100 == 0:\n",
    "      print('epoch:', i)\n",
    "      print('Loss from last step: %.3f' % loss_value)\n",
    "      print('Total running accuracy so far: %.3f' % accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab144741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a source dataset from your test data\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_set_x, test_set_y))\n",
    "test_dataset = test_dataset.batch(32)\n",
    "\n",
    "# This clears the internal state of the metric\n",
    "accuracy.reset_states()  \n",
    "\n",
    "for step, (x, y) in enumerate(test_dataset):\n",
    "  logits = model(x)\n",
    "  accuracy.update_state(y, logits)\n",
    "\n",
    "print('Final test accuracy: %.3f' % accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e908fcc",
   "metadata": {},
   "source": [
    "This exercise consists of the experimentation regarding the Cat vs Non-Cat model previously presented.\n",
    "<br><br>\n",
    "Evaluate the performance of the model (train and test) considering a different range of layers (1, 3, and 4) and units dimensions (16,32,64) <br>\n",
    "Experiment with other optimizers and evaluate different learning rate values (0.001, 0.01, 0.1, 1). What happens?<br>\n",
    "Has the size of mini-batch a huge influence under results? Make some experiments to support your findings.<br>\n",
    "Consider this reference and investigate different evaluation scenarios under the hyperparameter search.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8bdd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
